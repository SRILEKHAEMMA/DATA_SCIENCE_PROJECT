{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2wsfohCmC4f",
        "outputId": "4e471a70-7be4-49a3-f9be-927acf425fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Flattened DataFrame created. Shape: (405, 10)\n",
            "   paper_id preliminary_decision  review_id confidence evaluation orientation  \\\n",
            "0         1               accept          1          4          1           0   \n",
            "1         1               accept          2          4          1           1   \n",
            "\n",
            "  language remarks                                               text  \\\n",
            "0       es          - El artículo aborda un problema contingente y...   \n",
            "1       es          El artículo presenta recomendaciones prácticas...   \n",
            "\n",
            "     timespan  \n",
            "0  2010-07-05  \n",
            "1  2010-07-05  \n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# STEP 2: Import necessary libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# STEP 3: Load the JSON file\n",
        "json_path = \"/content/drive/MyDrive/reviews.json\"  # adjust if needed\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# STEP 4: Flatten the nested structure into a DataFrame\n",
        "flattened_reviews = []\n",
        "\n",
        "for paper in data['paper']:\n",
        "    paper_id = paper.get('id')\n",
        "    preliminary_decision = paper.get('preliminary_decision')\n",
        "\n",
        "    for review in paper['review']:\n",
        "        flattened_review = {\n",
        "            'paper_id': paper_id,\n",
        "            'preliminary_decision': preliminary_decision,\n",
        "            'review_id': review.get('id'),\n",
        "            'confidence': review.get('confidence'),\n",
        "            'evaluation': review.get('evaluation'),\n",
        "            'orientation': review.get('orientation'),\n",
        "            'language': review.get('lan'),\n",
        "            'remarks': review.get('remarks'),\n",
        "            'text': review.get('text'),\n",
        "            'timespan': review.get('timespan')\n",
        "        }\n",
        "        flattened_reviews.append(flattened_review)\n",
        "\n",
        "# STEP 5: Create the DataFrame\n",
        "df = pd.DataFrame(flattened_reviews)\n",
        "print(\"✅ Flattened DataFrame created. Shape:\", df.shape)\n",
        "print(df.head(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: TEXT CLEANING & PREPROCESSING"
      ],
      "metadata": {
        "id": "iRR6r2bJmfYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Install and import NLP libraries\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import string\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# STEP 7: Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# STEP 8: Load language models and stopwords\n",
        "nlp_es = spacy.load('es_core_news_sm')\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "spanish_stopwords = set(stopwords.words('spanish'))\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# STEP 9: Clean & preprocess text\n",
        "def preprocess_text(text, language='es'):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    if language == 'es':\n",
        "        doc = nlp_es(text)\n",
        "        stopwords_lang = spanish_stopwords\n",
        "    else:\n",
        "        doc = nlp_en(text)\n",
        "        stopwords_lang = english_stopwords\n",
        "\n",
        "    tokens = [token.lemma_ for token in doc if token.text not in stopwords_lang and not token.is_space]\n",
        "    return ' '.join(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NqUDYXBmRRd",
        "outputId": "adb5f935-06f4-4783-d389-173dda52b4ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "APPLY CLEANING + REMOVE EMPTY REVIEWS"
      ],
      "metadata": {
        "id": "hYg7fd5qnHe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Split data by language\n",
        "df_spanish = df[df['language'] == 'es'].copy()\n",
        "df_english = df[df['language'] == 'en'].copy()\n",
        "\n",
        "# STEP 11: Apply preprocessing\n",
        "df_spanish['cleaned_text'] = df_spanish['text'].apply(preprocess_text, language='es')\n",
        "df_english['cleaned_text'] = df_english['text'].apply(preprocess_text, language='en')\n",
        "\n",
        "# STEP 12: Combine preprocessed data\n",
        "df_cleaned = pd.concat([\n",
        "    df_spanish[['text', 'cleaned_text', 'language', 'confidence', 'evaluation', 'orientation']],\n",
        "    df_english[['text', 'cleaned_text', 'language', 'confidence', 'evaluation', 'orientation']]\n",
        "], ignore_index=True)\n",
        "\n",
        "# STEP 13: Create empty flag and drop truly empty reviews\n",
        "df_cleaned['empty_cleaned'] = df_cleaned['cleaned_text'].apply(lambda x: x.strip() == \"\")\n",
        "df_cleaned = df_cleaned[df_cleaned['empty_cleaned'] == False].reset_index(drop=True)\n",
        "\n",
        "print(\"✅ Cleaned reviews shape:\", df_cleaned.shape)\n",
        "print(df_cleaned[['cleaned_text', 'language', 'orientation']].head())\n",
        "\n",
        "\n",
        "\n",
        "#Length-Based Features\n",
        "\n",
        "# Add word count and char count\n",
        "df_cleaned['word_count'] = df_cleaned['cleaned_text'].apply(lambda x: len(str(x).split()))\n",
        "df_cleaned['char_count'] = df_cleaned['text'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# Preview\n",
        "print(df_cleaned[['cleaned_text', 'word_count', 'char_count']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYkzAnqYmnGG",
        "outputId": "22e952d1-a245-40db-acb2-eff47d2719d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned reviews shape: (399, 7)\n",
            "                                        cleaned_text language orientation\n",
            "0  artículo abordar problema contingente relevant...       es           0\n",
            "1  artículo presentar recomendación práctico desa...       es           1\n",
            "2  tema interesante poder ser mucho ayuda guía in...       es           1\n",
            "3  explicar forma ordenado didáctico experiencia ...       es           1\n",
            "4  autor describir metodología desarrollar forma ...       es           0\n",
            "                                        cleaned_text  word_count  char_count\n",
            "0  artículo abordar problema contingente relevant...          45         575\n",
            "1  artículo presentar recomendación práctico desa...          50         618\n",
            "2  tema interesante poder ser mucho ayuda guía in...         118        1259\n",
            "3  explicar forma ordenado didáctico experiencia ...         119        1350\n",
            "4  autor describir metodología desarrollar forma ...         162        1938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qi6bMCf5nKTG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}